{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High Level Computer Vision \n",
    "### Exercise 3: Convolutional Neural Networks\n",
    "\n",
    "**You may wirte your report in this `ipynb` file or submit the report as a PDF file.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Name = \"\"\n",
    "Matriculation_Number = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Exercise 3, you will implement a convolutional neural network to perform image classification and explore methods to improve the training performance and generalization of these networks.\n",
    "We will use the CIFAR-10 dataset as a benchmark for our networks, similar to the previous exercise. This dataset consists of 50000 training images of 32x32 resolution with 10 object classes, namely airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. The task is to implement a convolutional network to classify these images using the PyTorch library. The four questions are,\n",
    "\n",
    "- Implementing a convolutional neural network, training it, and visualizing its weights(Question 1).\n",
    "- Experiment with batch normalization and early stopping (Question 2).\n",
    "- Data augmentation and dropout to improve generalization (Question 3).\n",
    "- Implement transfer learning from an ImageNet-pretrained model (Question 4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Implement Convolutional Network (10 points)\n",
    "\n",
    "In this question, we will implement a five-layered convolutional neural network architecture as well as the loss function to train it. Refer to the comments in the code to the exact places where you need to fill in the code.\n",
    "\n",
    "<img src=\"./resources/fig1.png\" width=\"700\" align=left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Our architecture is shown in Fig 1. It has five convolution blocks. Each block is consist of convolution, max pooling, and ReLU operation in that order. We will use 3×3 kernels in all convolutional layers. Set the padding and stride of the convolutional layers so that they maintain the spatial dimensions. Max pooling operations are done with 2×2 kernels, with a stride of 2, thereby halving the spatial resolution each time. Finally, five stacking these five blocks leads to a 512 × 1 × 1 feature map. Classification is achieved by a fully connected layer. We will train convolutional neural networks on the CIFAR-10 dataset. Implement a class ConvNet to define the model described. The ConvNet takes 32 × 32 color images as inputs and has 5 hidden layers with 128, 512, 512, 512, 512 filters, and produces a 10-class classification. The code to train the model is already provided. Train the above model and report the training and validation accuracies. (5 points)\n",
    "\n",
    "b) Implement a function `PrintModelSize`, which calculates and prints the number of parameters of a neural network. `PrintModelSize` takes a model as input and returns the parameters of a model. This gives us a measure of model capacity. Report the number of parameters for the described model. (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./resources/fig2.png\" width=\"600\" align=left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Implement a function `VisualizeFilter`, which visualizes the filters of the first convolution layer implemented in Q1.a. In other words, you need to show 128 filters with size 3x3 as color images (since each filter has three input channels). Stack these into 3x3 color images into one large image. You can use the `imshow` function from the `matplotlib` library to visualize the weights. See an example in Fig. 2 Compare the filters before and after training. Do you see any patterns? (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wirte your report for Q1 in this cell.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Improve training of Convolutional Networks (10 points)\n",
    "\n",
    "a) Batch normalization is a widely used operation in neural networks, which will increase the speed of convergence and reach higher performance. Read the paper “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift” for more theoretical details.\n",
    "In practice, these operations are implemented in the most popular toolbox, such as PyTorch and TensorFlow. Add batch normalization in the model of Q1.a. Please keep other hyperparameters the same, but only add batch normalization. The ConvNet with batch normalization still uses the same class with Q1.a but different arguments. Check the code for details. In each block, the computations are in the order of convolution, batch normalization, pooling, and ReLU. Compare the loss curves and accuracy using batch normalization to its counterpart in Q1.a. (5 points)\n",
    "\n",
    "b) Early stopping is a method to alleviate overfitting. Instead of reporting the performance of the final model, early stopping also saves the best model on the validation set during training. Increase the training epochs to 50 in Q1.a and Q2.a, and compare the best model and latest model on the training set. Due to the randomness, you can train multiple times to verify and observe overfitting and early stopping. (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wirte your report for Q2 in this cell.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Improve generalization of Convolutional Networks (10 points)\n",
    "\n",
    "We saw in Q2 that the model can start over-fitting to the training set if we continue training for long. To prevent over-fitting, there are two main paradigms we can focus on. The first is to get more training data. This might be a difficult and expensive process involving significant. However, it is generally the most effective way to learn more general models. A cheaper alternative is to perform data augmentation. The second approach is to regularize the model. In the following sub-questions, we will experiment with each of these paradigms and measure the effect on the model generalization.\n",
    "\n",
    "a) Data augmentation is the process of creating more training data by applying certain transformations to the training set images. Usually, the underlying assumption is that the label of the image does not change under the applied transformations. This includes geometric transformations like translation, rotation, scaling, flipping, random cropping, and color transformations like greyscale, colorjitter. For every image in the training batch, a random transformation is sampled from the possible ones (e.g., a random number of pixels to translate the image by) and is applied to the image. While designing the data input pipeline, we must choose the hyper-parameters for these transformations (e.g., limits of translation or rotation) based on things we expect to see in the test-set/real world. Your task in this question is to implement the data augmentation for the CIFAR-10 classification task. Many of these transformations are implemented in the `torchvision.transforms` package. Familiarize your- self with the APIs of these transforms, and functions to compose multiple transforms or randomly sample them. Next, implement geometric and color space data augmentations for the CIFAR-10 dataset, by choos- ing the right functions and order of application. Tune the hyper-parameters of these data augmentations to improve the validation performance. You will need to train the model a bit longer (20-30 epochs) with data augmentation, as the training data is effectively larger now. Discuss which augmentations work well for you in the report. (6 points)\n",
    "\n",
    "b) Dropout is a popular scheme to regularize the model to improve generalization. The dropout layer works by setting the input activations randomly to zero at the output. You can implement Dropout by adding the `torch.nn.Dropout` layer between the conv blocks in your model. The layer has a single hyper-parameter $p$, which is the probability of dropping the input activations. High values of $p$ regularize the model heavily and decrease model capacity, but with low values, the model might overfit. Find the right hyper-parameter for $p$ by training the model for different values of $p$ and comparing training validation and validation accuracies. You can use the same parameter $p$ for all layers. You can also disable the data augmentation from the previous step while running this experiment, to clearly see the benefit of dropout. Show the plot of training and validation accuracies for different values of dropout (0.1 - 0.9) in the report. (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wirte your report for Q3 in this cell.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "[128, 512, 512, 512, 512, 512]\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "def weights_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        m.weight.data.normal_(0.0, 1e-3)\n",
    "        m.bias.data.fill_(0.)\n",
    "\n",
    "def update_lr(optimizer, lr):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "#--------------------------------\n",
    "# Device configuration\n",
    "#--------------------------------\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device: %s'%device)\n",
    "\n",
    "#--------------------------------\n",
    "# Hyper-parameters\n",
    "#--------------------------------\n",
    "input_size = 3\n",
    "num_classes = 10\n",
    "hidden_size = [128, 512, 512, 512, 512, 512]\n",
    "num_epochs = 20\n",
    "batch_size = 200\n",
    "learning_rate = 2e-3\n",
    "learning_rate_decay = 0.95\n",
    "reg=0.001\n",
    "num_training= 49000\n",
    "num_validation =1000\n",
    "norm_layer = None\n",
    "dropout = None\n",
    "print(hidden_size)\n",
    "\n",
    "\n",
    "#-------------------------------------------------\n",
    "# Load the CIFAR-10 dataset\n",
    "#-------------------------------------------------\n",
    "#################################################################################\n",
    "# TODO: Q3.a Chose the right data augmentation transforms with the right        #\n",
    "# hyper-parameters and put them in the data_aug_transforms variable             #\n",
    "#################################################################################\n",
    "data_aug_transforms = []\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "norm_transform = transforms.Compose(data_aug_transforms+[transforms.ToTensor(),\n",
    "                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                     ])\n",
    "test_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                     ])\n",
    "cifar_dataset = torchvision.datasets.CIFAR10(root='datasets/',\n",
    "                                           train=True,\n",
    "                                           transform=norm_transform,\n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='datasets/',\n",
    "                                          train=False,\n",
    "                                          transform=test_transform\n",
    "                                          )\n",
    "#-------------------------------------------------\n",
    "# Prepare the training and validation splits\n",
    "#-------------------------------------------------\n",
    "mask = list(range(num_training))\n",
    "train_dataset = torch.utils.data.Subset(cifar_dataset, mask)\n",
    "mask = list(range(num_training, num_training + num_validation))\n",
    "val_dataset = torch.utils.data.Subset(cifar_dataset, mask)\n",
    "\n",
    "#-------------------------------------------------\n",
    "# Data loader\n",
    "#-------------------------------------------------\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=False)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------\n",
    "# Convolutional neural network (Q1.a and Q2.a)\n",
    "# Set norm_layer for different networks whether using batch normalization\n",
    "#-------------------------------------------------\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, num_classes, norm_layer=None):\n",
    "        super(ConvNet, self).__init__()\n",
    "        #################################################################################\n",
    "        # TODO: Initialize the modules required to implement the convolutional layer    #\n",
    "        # described in the exercise.                                                    #\n",
    "        # For Q1.a make use of conv2d and relu layers from the torch.nn module.         #\n",
    "        # For Q2.a make use of BatchNorm2d layer from the torch.nn module.              #\n",
    "        # For Q3.b Use Dropout layer from the torch.nn module.                          #\n",
    "        #################################################################################\n",
    "        \n",
    "        layers = []\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        layers.append(nn.Conv2d(in_channels = input_size,\n",
    "                                out_channels = hidden_layers[0], \n",
    "                                kernel_size = 3,\n",
    "                                padding = 1)) # add p = 1 for k = 3 because: h - k + 2p + 1 = h - 3 + 2 + 1 = h\n",
    "        \n",
    "        # add batchnorm if norm_layer is not None\n",
    "        if norm_layer:\n",
    "            layers.append(nn.BatchNorm2d(hidden_layers[0]))\n",
    "            \n",
    "        layers.append(nn.MaxPool2d(stride= 2, kernel_size = 2))\n",
    "        layers.append(nn.ReLU())\n",
    "        \n",
    "        # Q3.a\n",
    "        if dropout:\n",
    "            layers.append(nn.Dropout(p = 0.3))\n",
    "        \n",
    "        for layer in range(len(hidden_layers) - 1):\n",
    "            layers.append(nn.Conv2d(in_channels = hidden_layers[layer],\n",
    "                                    out_channels = hidden_layers[layer + 1],\n",
    "                                    kernel_size = 3,\n",
    "                                    padding = 1))\n",
    "            # Q2.a\n",
    "            if norm_layer:\n",
    "                layers.append(nn.BatchNorm2d(hidden_layers[layer + 1]))\n",
    "            layers.append(nn.MaxPool2d(stride= 2, kernel_size = 2))\n",
    "            layers.append(nn.ReLU())\n",
    "            # Q3.a\n",
    "            if dropout:\n",
    "                layers.append(nn.Dropout(p = 0.3))\n",
    "\n",
    "        layers.append(nn.Flatten())\n",
    "        layers.append(nn.Linear(hidden_layers[-1], num_classes))\n",
    "        \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    def forward(self, x):\n",
    "        #################################################################################\n",
    "        # TODO: Implement the forward pass computations                                 #\n",
    "        #################################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        \n",
    "        out = self.layers(x)\n",
    "        \n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------\n",
    "# Calculate the model size (Q1.b)\n",
    "# if disp is true, print the model parameters, otherwise, only return the number of parameters.\n",
    "#-------------------------------------------------\n",
    "def PrintModelSize(model, disp=True):\n",
    "    #################################################################################\n",
    "    # TODO: Implement the function to count the number of trainable parameters in   #\n",
    "    # the input model. This useful to track the capacity of the model you are       #\n",
    "    # training                                                                      #\n",
    "    #################################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    model_sz = 0\n",
    "    for params in model.parameters():\n",
    "        if disp:\n",
    "            print(params)\n",
    "        model_sz += params.numel()\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    return model_sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------\n",
    "# Calculate the model size (Q1.c)\n",
    "# visualize the convolution filters of the first convolution layer of the input model\n",
    "#-------------------------------------------------\n",
    "def VisualizeFilter(model):\n",
    "    #################################################################################\n",
    "    # TODO: Implement the functiont to visualize the weights in the first conv layer#\n",
    "    # in the model. Visualize them as a single image fo stacked filters.            #\n",
    "    # You can use matlplotlib.imshow to visualize an image in python                #\n",
    "    #################################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    # ***** Source of inspiration: https://discuss.pytorch.org/t/visualize-feature-map/29597/2 \n",
    "    \n",
    "    # take the weights of first conv layer\n",
    "    weight_conv1 = model.layers[0].weight\n",
    "\n",
    "    # normalize\n",
    "    weight_conv1 = weight_conv1 - weight_conv1.min()\n",
    "    weight_conv1 = weight_conv1 / weight_conv1.max()\n",
    "\n",
    "    grid_image_weights = torchvision.utils.make_grid(weight_conv1.detach().cpu())\n",
    "    \n",
    "    plt.figure(figsize=(15, 15))\n",
    "    # move channel dim at the end for matplotlib\n",
    "    plt.imshow(grid_image_weights.permute(2, 1, 0))\n",
    "    plt.show()\n",
    "    \n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet(\n",
      "  (layers): Sequential(\n",
      "    (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): ReLU()\n",
      "    (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): ReLU()\n",
      "    (9): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (11): ReLU()\n",
      "    (12): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): ReLU()\n",
      "    (15): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): ReLU()\n",
      "    (18): Flatten(start_dim=1, end_dim=-1)\n",
      "    (19): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#======================================================================================\n",
    "# Q1.a: Implementing convolutional neural net in PyTorch\n",
    "#======================================================================================\n",
    "# In this question we will implement a convolutional neural networks using the PyTorch\n",
    "# library.  Please complete the code for the ConvNet class evaluating the model\n",
    "#--------------------------------------------------------------------------------------\n",
    "model = ConvNet(input_size, hidden_size, num_classes, norm_layer=norm_layer).to(device)\n",
    "# Q2.a - Initialize the model with correct batch norm layer\n",
    "\n",
    "model.apply(weights_init)\n",
    "# Print the model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10038282"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print model size\n",
    "#======================================================================================\n",
    "# Q1.b: Implementing the function to count the number of trainable parameters in the model\n",
    "#======================================================================================\n",
    "PrintModelSize(model, disp=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#======================================================================================\n",
    "# Q1.c: Implementing the function to visualize the filters in the first conv layers.\n",
    "# Visualize the filters before training\n",
    "#======================================================================================\n",
    "VisualizeFilter(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 100.00 MiB (GPU 0; 2.00 GiB total capacity; 1005.46 MiB already allocated; 37.44 MiB free; 1.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-5b44b7644e5d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;31m# Forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-65cfb88176db>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;31m# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;31m# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    440\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m--> 442\u001b[1;33m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    443\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 0; 2.00 GiB total capacity; 1005.46 MiB already allocated; 37.44 MiB free; 1.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "#======================================================================================\n",
    "# Q1.a: Implementing convolutional neural net in PyTorch\n",
    "#======================================================================================\n",
    "# In this question we will implement a convolutional neural networks using the PyTorch\n",
    "# library.  Please complete the code for the ConvNet class evaluating the model\n",
    "#--------------------------------------------------------------------------------------\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=reg)\n",
    "\n",
    "# Train the model\n",
    "lr = learning_rate\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Move tensors to the configured device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "    # Code to update the lr\n",
    "    lr *= learning_rate_decay\n",
    "    update_lr(optimizer, lr)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print('Validataion accuracy is: {} %'.format(100 * correct / total))\n",
    "        #################################################################################\n",
    "        # TODO: Q2.b Implement the early stopping mechanism to save the model which has #\n",
    "        # acheieved the best validation accuracy so-far.                                #\n",
    "        #################################################################################\n",
    "        best_model = None\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=reg)\n",
    "\n",
    "# Train the model\n",
    "lr = learning_rate\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Move tensors to the configured device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "    # Code to update the lr\n",
    "    lr *= learning_rate_decay\n",
    "    update_lr(optimizer, lr)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print('Validataion accuracy is: {} %'.format(100 * correct / total))\n",
    "        #################################################################################\n",
    "        # TODO: Q2.b Implement the early stopping mechanism to save the model which has #\n",
    "        # acheieved the best validation accuracy so-far.                                #\n",
    "        #################################################################################\n",
    "        best_model = None\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "model.eval()\n",
    "#################################################################################\n",
    "# TODO: Q2.b Implement the early stopping mechanism to load the weights from the#\n",
    "# best model so far and perform testing with this model.                        #\n",
    "#################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        if total == 1000:\n",
    "            break\n",
    "\n",
    "    print('Accuracy of the network on the {} test images: {} %'.format(total, 100 * correct / total))\n",
    "\n",
    "# Q1.c: Implementing the function to visualize the filters in the first conv layers.\n",
    "# Visualize the filters before training\n",
    "VisualizeFilter(model)\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: Use pretrained networks (10 points)\n",
    "\n",
    "It has become standard practice in computer vision tasks related to images to use a convolutional network pre-trained as the backbone feature extraction network and train new layers on top for the target task. In this question, we will implement such a model. We will use the `VGG_11_bn` network from the `torchvision.models` library as our backbone network. This model has been trained on ImageNet, achieving a top-5 error rate of 10.19%. It consists of 8 convolutional layers followed by adaptive average pooling and fully-connected layers to perform the classification. We will get rid of the average pooling and fully-connected layers from the `VGG_11_bn` model and attach our own fully connected layers to perform the CIFAR-10 classification.\n",
    "\n",
    "a) Instantiate a pretrained version of the `VGG_11_bn` model with ImageNet pre-trained weights. Add two fully connected layers on top, with Batch Norm and ReLU layers in between them, to build the CIFAR-10 10-class classifier. Note that you will need to set the correct mean and variance in the data-loader, to match the mean and variance the data was normalized with when the `VGG_11_bn` was trained. Train only the newly added layers while disabling gradients for the rest of the network. Each parameter in PyTorch has a required grad flag, which can be turned off to disable gradient computation for it. Get familiar with this gradient control mechanism in PyTorch and train the above model. As a reference point, you will see validation accuracies in the range (61-65%) if implemented correctly. (6 points)\n",
    "\n",
    "b) We can see that while the ImageNet features are useful, just learning the new layers does not yield better performance than training our own network from scratch. This is due to the domain-shift between the ImageNet dataset (224x224 resolution images) and the CIFAR-10 dataset (32x32 images). To improve the performance we can fine-tune the whole network on the CIFAR-10 dataset, starting from the ImageNet initialization. To do this, enable gradient computation to the rest of the network, and update all the model parameters. Additionally train a baseline model where the same entire network is trained from scratch, without loading the ImageNet weights. Compare the two models' training curves, validation, and testing performance in the report. (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wirte your report for Q4 in this cell.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        m.weight.data.normal_(0.0, 1e-3)\n",
    "        m.bias.data.fill_(0.)\n",
    "\n",
    "def update_lr(optimizer, lr):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "#--------------------------------\n",
    "# Device configuration\n",
    "#--------------------------------\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device: %s'%device)\n",
    "\n",
    "#--------------------------------\n",
    "# Hyper-parameters\n",
    "#--------------------------------\n",
    "input_size = 32 * 32 * 3\n",
    "layer_config= [512, 256]\n",
    "num_classes = 10\n",
    "num_epochs = 30\n",
    "batch_size = 200\n",
    "learning_rate = 1e-3\n",
    "learning_rate_decay = 0.99\n",
    "reg=0#0.001\n",
    "num_training= 49000\n",
    "num_validation =1000\n",
    "fine_tune = True\n",
    "pretrained=True\n",
    "\n",
    "data_aug_transforms = [transforms.RandomHorizontalFlip(p=0.5)]#, transforms.RandomGrayscale(p=0.05)]\n",
    "#-------------------------------------------------\n",
    "# Load the CIFAR-10 dataset\n",
    "#-------------------------------------------------\n",
    "# Q1,\n",
    "norm_transform = transforms.Compose(data_aug_transforms+[transforms.ToTensor(),\n",
    "                                     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "                                     ])\n",
    "cifar_dataset = torchvision.datasets.CIFAR10(root='datasets/',\n",
    "                                           train=True,\n",
    "                                           transform=norm_transform,\n",
    "                                           download=False)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='datasets/',\n",
    "                                          train=False,\n",
    "                                          transform=norm_transform\n",
    "                                          )\n",
    "#-------------------------------------------------\n",
    "# Prepare the training and validation splits\n",
    "#-------------------------------------------------\n",
    "mask = list(range(num_training))\n",
    "train_dataset = torch.utils.data.Subset(cifar_dataset, mask)\n",
    "mask = list(range(num_training, num_training + num_validation))\n",
    "val_dataset = torch.utils.data.Subset(cifar_dataset, mask)\n",
    "\n",
    "#-------------------------------------------------\n",
    "# Data loader\n",
    "#-------------------------------------------------\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=False)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VggModel(nn.Module):\n",
    "    def __init__(self, n_class, fine_tune, pretrained=True):\n",
    "        super(VggModel, self).__init__()\n",
    "        #################################################################################\n",
    "        # TODO: Build the classification network described in Q4 using the              #\n",
    "        # models.vgg11_bn network from torchvision model zoo as the feature extraction  #\n",
    "        # layers and two linear layers on top for classification. You can load the      #\n",
    "        # pretrained ImageNet weights based on the pretrained flag. You can enable and  #\n",
    "        # disable training the feature extraction layers based on the fine_tune flag.   #\n",
    "        #################################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    def forward(self, x):\n",
    "        #################################################################################\n",
    "        # TODO: Implement the forward pass computations                                 #\n",
    "        #################################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        return out\n",
    "\n",
    "# Initialize the model for this run\n",
    "model= VggModel(num_classes, fine_tune, pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model we just instantiated\n",
    "print(model)\n",
    "\n",
    "#################################################################################\n",
    "# TODO: Only select the required parameters to pass to the optimizer. No need to#\n",
    "# update parameters which should be held fixed (conv layers).                   #\n",
    "#################################################################################\n",
    "print(\"Params to learn:\")\n",
    "if fine_tune:\n",
    "    params_to_update = []\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "else:\n",
    "    params_to_update = model.parameters()\n",
    "    for name,param in model.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params_to_update, lr=learning_rate, weight_decay=reg)\n",
    "\n",
    "# Train the model\n",
    "lr = learning_rate\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Move tensors to the configured device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "    # Code to update the lr\n",
    "    lr *= learning_rate_decay\n",
    "    update_lr(optimizer, lr)\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        #################################################################################\n",
    "        # TODO: Q2.b Use the early stopping mechanism from previous questions to save   #\n",
    "        # the model which has acheieved the best validation accuracy so-far.            #\n",
    "        #################################################################################\n",
    "        best_model = None\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "        print('Validataion accuracy is: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "# TODO: Use the early stopping mechanism from previous question to load the     #\n",
    "# weights from the best model so far and perform testing with this model.       #\n",
    "#################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        if total == 1000:\n",
    "            break\n",
    "\n",
    "    print('Accuracy of the network on the {} test images: {} %'.format(total, 100 * correct / total))\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
